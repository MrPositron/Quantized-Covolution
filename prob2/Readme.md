# Readme

The command to run the code is as follows:  
`./convolution input_tensor.bin kernel_tensor.bin [32/16/8]`

The code will save the result in output_tensor.bin. It will also compute NRMSE (Normalized Root Mean Square Error). Therefore, save the output files from Prob 1 and move to the local directory of Prob 2. Then, in the code specify the name of the target binary file (i.e. from Prob 1) to compute NRMSE.

* Finding the best constant S is very arduous process. There are several constraints that have to be maintained. First is that multiplication by S should not cause the overflow. For example, in int8_t multiplication 10 by 100 cannot be done (int8_t allows values only in the range of -127 to 128). In matrix multiplication, summation is performed over vales produced by multiplication. Thus, the second problem is that summation should not overflow. This is the most challenging problem. If every multiplication leads to the maximum value of the product then constant S have to be small. However, the probability that multiplication of the maximum value of the input tensor and kernel tensor is really small. Thus, it was not possible to derive efficient constants theoretically. 

* When I implemented a quantized version of the convolution operation I observed that int8_t allows values only between -127 and 128. However, during convolution we are doing summation over a really large set of numbers. For example, in the files from group2/1 we are making summation over 1152 elements and the values of the input_tensor are in range from -25 to 25. Thus, when we use any constant bigger than 1 for the convolution it will always lead to overflow (we assume that we multiply some constant to kernel values to make them bigger than 1, otherwise they will always be zero). The solution to this problem was to store values in int8_t and perform multiplication in int8_t. Then sum and store them using int16_t. For implementation with int16_t and int32_t we did not have to use that strategy. int16_t and int32_t are pretty big to use only one data type. When looking to the kernel and input I observed that they have different variants. Thus I used different constants for them. CON1 for the input data and CON2 for the kernel data. Keeping in mind the constraints I mentioned before, I tried different values for CON1 and CON2.

* As I observed there is no quantization overhead. When transforming kernels and input to the desired format for the matrix multiplication we casted them to the integers. Thus it saved time for the memory allocation. Furthermore, the saved time compensated the overhead when we transformed integers back to floating numbers.
